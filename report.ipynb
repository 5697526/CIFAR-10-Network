{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d600ce97",
   "metadata": {},
   "source": [
    "代码保存在我的[github](https://github.com/5697526/CIFAR-10-Network.git)\n",
    "\n",
    "模型保存在我的网盘："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0f541",
   "metadata": {},
   "source": [
    "## 一、实验目的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409bc62",
   "metadata": {},
   "source": [
    "本项目旨在 CIFAR - 10 数据集上训练神经网络模型，以实现图像分类任务的性能优化。通过构建不同的神经网络模型，运用多种优化策略和超参数搜索方法，以提高模型在 CIFAR - 10 数据集上的分类准确率。同时，项目还包含了对网络的可视化分析，帮助理解模型的学习过程和决策机制。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f41a7",
   "metadata": {},
   "source": [
    "## 二、数据集介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b83675",
   "metadata": {},
   "source": [
    "### 2.1 数据集来源  \n",
    "\n",
    "\n",
    "本次实验使用的 CIFAR - 10 数据集是一个广泛用于图像分类研究的公开数据集，该数据集可从 [CIFAR - 10 ](https://www.cs.toronto.edu/~kriz/cifar.html)下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da55fd0a",
   "metadata": {},
   "source": [
    "### 2.2 数据集内容\n",
    "\n",
    "CIFAR - 10 数据集总共包含 60,000 张 32×32 像素的彩色图像，涵盖有10个不同的类别：飞机（Airplane），汽车（Automobile），鸟（Bird），猫（Cat），鹿（Deer），狗（Dog），青蛙（Frog），马（Horse），船（Ship）和卡车（Truck）。数据集被划分为训练集和测试集两部分，其中训练集包含 50,000 张图像，每个类别有 5,000 张，测试集包含 10,000 张图像，每个类别有 1,000 张。图像采用 RGB 三通道模式，每个通道是一个 32×32 的矩阵，矩阵中的每个元素表示该通道在对应像素位置的颜色强度，取值范围为 0 - 255。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0548d2",
   "metadata": {},
   "source": [
    "### 2.3 数据格式与存储\n",
    "\n",
    "CIFAR - 10 数据集以二进制文件的形式存储，分为多个批次。训练集被分成 5 个数据批次（data_batch_1 - data_batch_5），每个批次包含 10,000 张图像。测试集则存储在一个单独的文件（test_batch）中。每个数据批次文件包含图像数据和对应的标签，图像数据以一维数组的形式存储。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552899e0",
   "metadata": {},
   "source": [
    "## 三、实验准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96038da",
   "metadata": {},
   "source": [
    "### 3.1 依赖安装\n",
    "\n",
    "确保已经安装了 Python 环境，并且安装必要的 Python 库。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7ce4f",
   "metadata": {},
   "source": [
    "### 3.2 数据加载\n",
    "\n",
    "`utils\\dataloader.py`使用 torchvision 包加载 CIFAR - 10 数据集，并进行预处理，包括将图像转换为张量和归一化操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bfe93",
   "metadata": {},
   "source": [
    "## 四、项目构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892af28",
   "metadata": {},
   "source": [
    "### 4.1 必需组件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd8ba3",
   "metadata": {},
   "source": [
    "#### (1) 全连接层 Fully-Connected layer\n",
    "\n",
    "全连接层用于最终的分类任务，将卷积层提取的特征映射转换为类别概率。\n",
    "\n",
    "在 models/model.py 中，BasicCNN 模型的全连接层代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        # ...\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ...\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26374a2",
   "metadata": {},
   "source": [
    "在 models/model_optimized.py 中，ConfigurableCNN 模型的全连接层代码如下，其配置可以根据不同的 fc_units 进行调整："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3345788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, filter_config=None, fc_units=None):\n",
    "        # ...\n",
    "        if fc_units is None:\n",
    "            fc_units = [512, num_classes]\n",
    "        self.fc1 = nn.Linear(feature_dim, fc_units[0])\n",
    "        self.fc2 = nn.Linear(fc_units[0], fc_units[1])\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ...\n",
    "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8fb024",
   "metadata": {},
   "source": [
    "#### (2) 二维卷积层 2D convolutional layer\n",
    "\n",
    "二维卷积层用于特征提取。\n",
    "\n",
    "在 BasicCNN 模型中，定义了三个二维卷积层 self.conv1、self.conv2 和 self.conv3："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3389fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5cc1e8",
   "metadata": {},
   "source": [
    "在 ConfigurableCNN 模型中，使用 nn.ModuleList 动态创建二维卷积层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, filter_config=None, fc_units=None):\n",
    "        # ...\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_channels = 3\n",
    "        for i, out_channels in enumerate(filter_config):\n",
    "            self.convs.append(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            in_channels = out_channels\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a563efc",
   "metadata": {},
   "source": [
    "#### (3) 二维池化层 2D pooling layer\n",
    "\n",
    "二维池化层通常使用 nn.MaxPool2d 进行下采样，其作用是减少特征图的尺寸，同时保留重要的特征信息，降低计算量和模型的复杂度。\n",
    "\n",
    "二维池化层在 BasicCNN 和 ConfigurableCNN 模型中都有使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c638cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        # ...\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        # ...\n",
    "\n",
    "\n",
    "class ConfigurableCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, filter_config=None, fc_units=None):\n",
    "        # ...\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.pool(self.relu(self.bns[i](self.convs[i](x))))\n",
    "            # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c5452",
   "metadata": {},
   "source": [
    "#### (4) 激活函数 Activations\n",
    "\n",
    "激活函数为神经网络引入了非线性特性，使得网络能够学习到更复杂的模式。\n",
    "\n",
    "ReLU 激活函数在 BasicCNN 和 ConfigurableCNN 模型中都有使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd938970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        # ...\n",
    "        self.relu = nn.ReLU()\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        # ...\n",
    "\n",
    "\n",
    "class ConfigurableCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, filter_config=None, fc_units=None):\n",
    "        # ...\n",
    "        self.relu = nn.ReLU()\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.pool(self.relu(self.bns[i](self.convs[i](x))))\n",
    "            # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c37aab",
   "metadata": {},
   "source": [
    "### 4.2 可选组件\n",
    "\n",
    "#### (1) 批量归一化层 Batch-Norm layer\n",
    "\n",
    "批量归一化层（nn.BatchNorm2d）用于加速模型的训练过程并提高模型的稳定性。在 model_optimized.py 中的 ConfigurableCNN 模型使用了该层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b43a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, filter_config=None, fc_units=None):\n",
    "        # ...\n",
    "        self.bns = nn.ModuleList()\n",
    "        in_channels = 3\n",
    "        for i, out_channels in enumerate(filter_config):\n",
    "            self.convs.append(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            self.bns.append(nn.BatchNorm2d(out_channels))\n",
    "            in_channels = out_channels\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.pool(self.relu(self.bns[i](self.convs[i](x))))\n",
    "            # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273dfbb4",
   "metadata": {},
   "source": [
    "#### (2) Dropout 层\n",
    "\n",
    "Dropout 层是一种防止模型过拟合的有效方法。在 model_optimized.py 中的 ConfigurableCNN 模型使用了 nn.Dropout 层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, filter_config=None, fc_units=None):\n",
    "        # ...\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.pool(self.relu(self.bns[i](self.convs[i](x))))\n",
    "            x = self.dropout(x)\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f82bba",
   "metadata": {},
   "source": [
    "#### (3) 残差连接 Residual Connection\n",
    "\n",
    "残差连接有助于解决深度神经网络中的梯度消失和梯度爆炸问题。在 model.py 中的 ResNet 模型使用了残差连接，通过 ResidualBlock 类实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        # ...\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)  # 残差连接\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        # ...\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7d8ea8",
   "metadata": {},
   "source": [
    "### 4.3 必需优化策略\n",
    "\n",
    "#### (1) 不同数量的神经元 / 滤波器\n",
    "\n",
    "在 model_optimized.py 中，定义了不同的滤波器配置方案 FILTER_CONFIGS 和全连接层配置 FC_UNITS_CONFIGS，可以通过调整这些配置来改变模型的容量和复杂度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1dfc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_CONFIGS = {\n",
    "    \"small\": [16, 32, 64],       # 小容量网络\n",
    "    \"medium\": [32, 64, 128],     # 中等容量网络(原始配置)\n",
    "    \"large\": [64, 128, 256],     # 大容量网络\n",
    "    \"wide\": [32, 128, 256],      # 宽卷积网络\n",
    "    \"deep\": [32, 64, 128, 256]   # 深度卷积网络，4层卷积\n",
    "}\n",
    "\n",
    "FC_UNITS_CONFIGS = {\n",
    "    \"default\": [512, 10],        # 原始配置\n",
    "    \"simplified\": [256, 10],     # 简化全连接层\n",
    "    \"complex\": [1024, 512, 10]   # 复杂全连接层\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827fb7d",
   "metadata": {},
   "source": [
    "#### (2) 不同的损失函数和正则化\n",
    "\n",
    "在 utils/loss_utils.py 中，定义了多种损失函数和正则化方法。\n",
    "\n",
    "**焦点损失（Focal Loss）：**\n",
    "\n",
    "主要用于解决数据集中类别不平衡的问题。在类别不平衡的情况下，模型可能会偏向于多数类，而焦点损失通过降低容易分类样本的权重，使得模型更加关注难分类的样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9715e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729675c",
   "metadata": {},
   "source": [
    "**标签平滑损失（Label Smoothing Loss）：**\n",
    "\n",
    "用于防止模型过拟合。传统的交叉熵损失会让模型对正确标签的预测概率趋近于 1，而标签平滑损失通过将真实标签的概率分布进行平滑，使得模型学习到更泛化的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7588f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=10, smoothing=0.1, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = inputs.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(inputs)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, targets.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * inputs, dim=self.dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860b7ec",
   "metadata": {},
   "source": [
    "**L1 正则化（L1 Regularization）：**\n",
    "\n",
    "用于防止模型过拟合，通过在损失函数中添加模型权重的 L1 范数，使得模型的权重更加稀疏，即部分权重趋近于 0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06002602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Regularization(nn.Module):\n",
    "    def __init__(self, model, lambda_l1=1e-4):\n",
    "        super(L1Regularization, self).__init__()\n",
    "        self.model = model\n",
    "        self.lambda_l1 = lambda_l1\n",
    "\n",
    "    def forward(self):\n",
    "        l1_loss = 0\n",
    "        for param in self.model.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 1:  \n",
    "                l1_loss += torch.norm(param, 1)\n",
    "        return self.lambda_l1 * l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9dc13",
   "metadata": {},
   "source": [
    "**L2 正则化（L2 Regularization，权重衰减）：**\n",
    "\n",
    "同样用于防止模型过拟合，通过在损失函数中添加模型权重的 L2 范数，使得模型的权重不会过大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Regularization(nn.Module):\n",
    "    def __init__(self, model, lambda_l2=5e-4):\n",
    "        super(L2Regularization, self).__init__()\n",
    "        self.model = model\n",
    "        self.lambda_l2 = lambda_l2\n",
    "\n",
    "    def forward(self):\n",
    "        l2_loss = 0\n",
    "        for param in self.model.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 1:  \n",
    "                l2_loss += torch.norm(param, 2)\n",
    "        return self.lambda_l2 * l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6dd93",
   "metadata": {},
   "source": [
    "**组合损失函数（CombinedLoss）：**\n",
    "\n",
    "支持同时使用多种损失函数和正则化项，将基础损失函数的结果与 L1 和 L2 正则化项的结果相加。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685266f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, base_loss, l1_reg=None, l2_reg=None):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = self.base_loss(outputs, targets)\n",
    "        if self.l1_reg:\n",
    "            loss += self.l1_reg()\n",
    "        if self.l2_reg:\n",
    "            loss += self.l2_reg()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f8d73",
   "metadata": {},
   "source": [
    "#### (3) 不同的激活函数\n",
    "\n",
    "项目中使用了五种不同的激活函数，包括RELU、Swish、Mish、SELU、GELU，并使用激活函数工厂 get_activation 根据配置创建不同的激活函数。\n",
    "\n",
    "在 models/activations.py 中定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "\n",
    "class SELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.selu(x)\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(2 / torch.tensor(torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "def get_activation(activation_type='relu'):\n",
    "    if activation_type == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation_type == 'swish':\n",
    "        return Swish()\n",
    "    elif activation_type == 'mish':\n",
    "        return Mish()\n",
    "    elif activation_type == 'selu':\n",
    "        return SELU()\n",
    "    elif activation_type == 'gelu':\n",
    "        return GELU()\n",
    "    else:\n",
    "        raise ValueError(f\"不支持的激活函数类型: {activation_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30781e",
   "metadata": {},
   "source": [
    "### 4.4 可选优化策略\n",
    "\n",
    "#### (1) 不同的优化器\n",
    "\n",
    "使用 torch.optim 提供的不同优化器，如 SGD、Adam 和 RMSprop。在 train.py、train_optimized.py 和 train_custom_optimizer.py 中，根据不同的配置选择不同的优化器。\n",
    "\n",
    "在 train.py 中使用 SGD 优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de481103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ...\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        momentum=0.9,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92801c",
   "metadata": {},
   "source": [
    "在 train_optimized.py 中使用 Adam 优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ...\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config[\"learning_rate\"],\n",
    "            betas=(config[\"beta1\"], config[\"beta2\"]),\n",
    "            eps=config[\"eps\"],\n",
    "            weight_decay=config[\"weight_decay\"] if config[\"use_l2\"] else 0\n",
    "        )\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a2dc1",
   "metadata": {},
   "source": [
    "#### (2) 自定义优化器\n",
    "\n",
    "在 utils/custom_optimizer.py 中实现了自定义优化器 CustomOptimizer，结合了动量、二阶矩估计和权重衰减等功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr=0.01, beta1=0.9, beta2=0.999,\n",
    "                 epsilon=1e-8, weight_decay=0, momentum=0, nesterov=False):\n",
    "        # ...\n",
    "        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, epsilon=epsilon,\n",
    "                        weight_decay=weight_decay, momentum=momentum, nesterov=nesterov)\n",
    "        super(CustomOptimizer, self).__init__(params, defaults)\n",
    "    # ...\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        # ...\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            epsilon = group['epsilon']\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "                # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159702e2",
   "metadata": {},
   "source": [
    "### 4.5 网络可视化\n",
    "\n",
    "#### (1) 滤波器可视化\n",
    "\n",
    "在 visualize_network.py 中，使用 visualize_filters 函数可视化卷积层的滤波器，帮助理解模型学习到的特征模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ...\n",
    "    print(\"可视化卷积层滤波器...\")\n",
    "    fig = visualize_filters(model, 'conv1', num_cols=8, figsize=(12, 12))\n",
    "    plt.savefig('./visualizations/conv1_filters.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = visualize_filters(model, 'conv2', num_cols=8, figsize=(12, 12))\n",
    "    plt.savefig('./visualizations/conv2_filters.png')\n",
    "    plt.close(fig)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b771798",
   "metadata": {},
   "source": [
    "#### (2) 特征图可视化\n",
    "\n",
    "同样在 visualize_network.py 中，使用 visualize_feature_maps 函数可视化输入图像通过指定卷积层后的特征图，展示模型在不同层提取的特征信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ...\n",
    "    print(\"可视化特征图...\")\n",
    "    fig = visualize_feature_maps(\n",
    "        model, 'conv1', images[0], num_cols=8, figsize=(12, 12))\n",
    "    plt.savefig('./visualizations/conv1_feature_maps.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = visualize_feature_maps(\n",
    "        model, 'conv2', images[0], num_cols=8, figsize=(12, 12))\n",
    "    plt.savefig('./visualizations/conv2_feature_maps.png')\n",
    "    plt.close(fig)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e997210",
   "metadata": {},
   "source": [
    "#### (3) 显著性图可视化\n",
    "\n",
    "使用 visualize_saliency 函数可视化模型关注的图像区域，帮助理解模型的决策过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19391958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ...\n",
    "    print(\"可视化显著性图...\")\n",
    "    for i in range(min(5, len(images))):  # 可视化前5张图像\n",
    "        fig = visualize_saliency(\n",
    "            model, images[i], target_class=labels[i].item())\n",
    "        plt.savefig(f'./visualizations/saliency_map_{i}.png')\n",
    "        plt.close(fig)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c92e2b",
   "metadata": {},
   "source": [
    "#### (4) 损失景观可视化\n",
    "\n",
    "使用 plot_loss_landscape 函数可视化模型的损失景观，展示模型在参数空间中的损失变化情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ...\n",
    "    print(\"可视化损失景观...\")\n",
    "    fig = plot_loss_landscape(\n",
    "        model, testloader, device, num_points=10, figsize=(10, 8))\n",
    "    plt.savefig('./visualizations/loss_landscape.png')\n",
    "    plt.close(fig)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afb442",
   "metadata": {},
   "source": [
    "#### (4) Grad - CAM 可视化\n",
    "\n",
    "在 visualize_grad_cam.py 中，使用 visualize_grad_cam 函数可视化模型的 Grad - CAM 结果，突出显示对模型决策最重要的图像区域。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ...\n",
    "    for i in range(10):\n",
    "        images, labels = dataiter.next()\n",
    "        image = images[0].to(device)\n",
    "        fig = visualize_grad_cam(\n",
    "            model,\n",
    "            image,\n",
    "            target_layer='conv3',\n",
    "            class_names=classes,\n",
    "            figsize=(12, 4)\n",
    "        )\n",
    "        plt.savefig(f'./grad_cam_visualizations/grad_cam_{i}.png')\n",
    "        plt.close(fig)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd53fb",
   "metadata": {},
   "source": [
    "## 五、实验过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de447c",
   "metadata": {},
   "source": [
    "### 5.1 超参数搜索\n",
    "\n",
    "使用 hyperparameter_search_main.py 脚本进行超参数搜索，可以选择随机搜索或贝叶斯优化方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='CIFAR-10 超参数搜索')\n",
    "    parser.add_argument('--method', type=str, default='random', choices=['random', 'bayesian'],\n",
    "                        help='超参数搜索方法 (random 或 bayesian)')\n",
    "    parser.add_argument('--trials', type=int, default=20,\n",
    "                        help='试验次数')\n",
    "    parser.add_argument('--initial_random', type=int, default=5,\n",
    "                        help='贝叶斯优化的初始随机试验次数')\n",
    "    parser.add_argument('--save_dir', type=str, default='./hyperparameter_search',\n",
    "                        help='保存搜索结果的目录')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    trainloader, testloader = get_dataloaders(batch_size=32)\n",
    "\n",
    "    if args.method == 'random':\n",
    "        search = RandomSearch(\n",
    "            model_builder=ConfigurableCNN,\n",
    "            dataloaders=(trainloader, testloader),\n",
    "            num_trials=args.trials,\n",
    "            save_dir=args.save_dir\n",
    "        )\n",
    "    else:  # bayesian\n",
    "        search = BayesianOptimization(\n",
    "            model_builder=ConfigurableCNN,\n",
    "            dataloaders=(trainloader, testloader),\n",
    "            num_trials=args.trials,\n",
    "            num_initial_random=args.initial_random,\n",
    "            save_dir=args.save_dir\n",
    "        )\n",
    "\n",
    "    search.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51695ae",
   "metadata": {},
   "source": [
    "### 5.2 模型训练\n",
    "\n",
    "可以使用不同的训练脚本来训练模型，如 train.py、train_optimized.py、train_component_optimizers.py 和 train_custom_optimizer.py。这些脚本中包含了不同的优化策略和超参数配置。\n",
    "\n",
    "**train.py：**\n",
    "\n",
    "使用基本模型 BasicCNN 进行训练，采用了随机梯度下降（SGD）优化器和交叉熵损失函数。\n",
    "\n",
    "**train_optimized.py：**\n",
    "\n",
    "使用优化模型 ConfigurableCNN 进行训练，采用了 Adam 优化器和组合损失函数。\n",
    "\n",
    "**train_component_optimizers.py：**\n",
    "\n",
    "使用优化模型 ConfigurableCNN 进行训练，为模型的不同层（卷积层和全连接层）使用不同的优化器。\n",
    "\n",
    "**train_custom_optimizer.py：**\n",
    "\n",
    "使用优化模型 ConfigurableCNN ，使用自定义的优化器CustomOptimizer进行训练。\n",
    "\n",
    "此处展示 train_optimized.py 的部分示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7740de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    config = {\n",
    "        \"filter_config\": \"wide\",\n",
    "        \"fc_units\": \"default\",\n",
    "        \"activation\": \"mish\",\n",
    "        \"loss_type\": \"ls\",\n",
    "        \"focal_alpha\": 1,\n",
    "        \"focal_gamma\": 2,\n",
    "        \"ls_smoothing\": 0,\n",
    "        \"use_l1\": False,\n",
    "        \"lambda_l1\": 1e-4,\n",
    "        \"use_l2\": True,\n",
    "        \"lambda_l2\": 0.0001,\n",
    "        \"num_epochs\": 30,\n",
    "        \"batch_size\": 128,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"momentum\": 0,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.999,\n",
    "        \"eps\": 1e-8\n",
    "    }\n",
    "\n",
    "    trainloader, testloader = get_dataloaders(config[\"batch_size\"])\n",
    "    model = ConfigurableCNN(\n",
    "        num_classes=10,\n",
    "        filter_config=FILTER_CONFIGS[config[\"filter_config\"]],\n",
    "        fc_units=FC_UNITS_CONFIGS[config[\"fc_units\"]]\n",
    "    ).to(device)\n",
    "\n",
    "    base_loss = get_loss_function(\n",
    "        loss_type=config[\"loss_type\"],\n",
    "        alpha=config[\"focal_alpha\"],\n",
    "        gamma=config[\"focal_gamma\"],\n",
    "        smoothing=config[\"ls_smoothing\"]\n",
    "    )\n",
    "    l1_reg = L1Regularization(\n",
    "        model, config[\"lambda_l1\"]) if config[\"use_l1\"] else None\n",
    "    l2_reg = L2Regularization(\n",
    "        model, config[\"lambda_l2\"]) if config[\"use_l2\"] else None\n",
    "    criterion = CombinedLoss(base_loss, l1_reg, l2_reg)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config[\"learning_rate\"],\n",
    "        betas=(config[\"beta1\"], config[\"beta2\"]),\n",
    "        eps=config[\"eps\"],\n",
    "        weight_decay=config[\"weight_decay\"] if config[\"use_l2\"] else 0\n",
    "    )\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config[\"num_epochs\"])\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, device, trainloader,\n",
    "                           criterion, optimizer, epoch)\n",
    "        test_acc = evaluate(model, device, testloader)\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save({\n",
    "                'config': config,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'accuracy': test_acc\n",
    "            }, './results/best_model.pth')\n",
    "\n",
    "        scheduler.step()\n",
    "        print(\n",
    "            f'Epoch {epoch+1}/{config[\"num_epochs\"]}, Train Loss: {train_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "    print(f'Best test accuracy: {best_acc:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e15e68f",
   "metadata": {},
   "source": [
    "### 5.3 结果分析\n",
    "\n",
    "使用 analyze_search_results.py 脚本分析超参数搜索结果，包括最佳配置的选择、准确率分布的可视化和参数相关性分析。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results_file, save_dir='./search_analysis'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    df = pd.read_csv(results_file)\n",
    "\n",
    "    best_config = df.loc[df['best_accuracy'].idxmax()]\n",
    "    print(\"最佳配置:\")\n",
    "    print(best_config)\n",
    "    best_config.to_csv(os.path.join(save_dir, 'best_config.csv'))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['best_accuracy'], kde=True)\n",
    "    plt.title('准确率分布')\n",
    "    plt.xlabel('准确率 (%)')\n",
    "    plt.ylabel('试验次数')\n",
    "    plt.savefig(os.path.join(save_dir, 'accuracy_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "    categorical_params = ['filter_config', 'fc_units',\n",
    "                          'activation', 'optimizer', 'loss_type']\n",
    "    for param in categorical_params:\n",
    "        if param in df.columns:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.boxplot(x=param, y='best_accuracy', data=df)\n",
    "            plt.title(f'{param} 对准确率的影响')\n",
    "            plt.savefig(os.path.join(save_dir, f'{param}_vs_accuracy.png'))\n",
    "            plt.close()\n",
    "\n",
    "    continuous_params = ['learning_rate', 'momentum', 'weight_decay', 'batch_size',\n",
    "                         'focal_alpha', 'focal_gamma', 'ls_smoothing']\n",
    "    for param in continuous_params:\n",
    "        if param in df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.scatterplot(x=param, y='best_accuracy', data=df)\n",
    "            plt.title(f'{param} 对准确率的影响')\n",
    "            plt.savefig(os.path.join(save_dir, f'{param}_vs_accuracy.png'))\n",
    "            plt.close()\n",
    "\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "    corr = numeric_df.corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('参数相关性分析')\n",
    "    plt.savefig(os.path.join(save_dir, 'correlation_heatmap.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"分析完成，结果保存在 {save_dir} 目录中\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c118d988",
   "metadata": {},
   "source": [
    "项目还提供了网络可视化的功能，包括滤波器可视化、特征图可视化、显著性图可视化、损失景观可视化和 Grad - CAM 可视化，在报告前面的内容里有提及如何实现。\n",
    "\n",
    "运行 visualize_network.py 和 visualize_grad_cam.py 代码即可进行所有可视化操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3dddd1",
   "metadata": {},
   "source": [
    "## 六、实验结果\n",
    "\n",
    "### 6.1 实验配置\n",
    "\n",
    "经过"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
